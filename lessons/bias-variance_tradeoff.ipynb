{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Bias-Variance Tradeoff\n",
    "\n",
    "In this short notebook, we'll explore a phenomenon often referred to as the _bias-variance tradeoff_.  The idea is that  a machine learning model is naturally _biased_ in that it makes assumptions about the dataset.  A linear regression model, for example, makes the assumption that your data is linear.  Contrast this with a model's built-in flexibility, or _variance_.  This notion captures a model's ability to modify its output to match the training data.  A high-degree polynomial regression is extremely variant.  Let's see this in action."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import LinearRegression\n",
    "import math\n",
    "from statistics import mean\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "N = 40         # max poly degree\n",
    "NOISE = .5     # amount of noise in the data\n",
    "\n",
    "plt.style.use('fivethirtyeight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we create and plot a sample dataset.  I build this by adding noise to a function of _ground truth_: this is the true function that my data is based upon.  In a real-life situation, you wouldn't have access to this _ground truth_ function, and so we hope to approximate it with our machine learning models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.arange(-1,1,.01)\n",
    "\n",
    "true_model = lambda x:  3*x**2 + 5 * np.vectorize(math.sin)(x**(4) + 3) +x\n",
    "y_true = true_model(x) \n",
    "\n",
    "y = y_true + (NOISE * np.random.randn(y_true.size))\n",
    "\n",
    "plt.scatter(x,y)\n",
    "plt.plot(x, y_true, 'k--')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Okay, so let's fit a line to this, and also a quadratic, a cubic, quartic, _etc._; all the polynomials up to degree `N`.  First, we extend our dataset to include these features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame({'x':x})\n",
    "for i in range(2,N+1):\n",
    "    df['x^' + str(i)] = x**i\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we generate each of the models, fit them, and make preditions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Xs = [df[list(df.columns[:i])].values for i in range(1,df.shape[1])]\n",
    "models = [LinearRegression().fit(X, y) for X in Xs]\n",
    "predictions = [model.predict(X) for X,model in zip(Xs,models)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's visualize the models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_color_value(i):\n",
    "    value = str(hex(i)[2:]) \n",
    "    if len(value) < 2:\n",
    "        return '0' + value\n",
    "    return value\n",
    "\n",
    "colors = ['#' + make_color_value(55+i) + make_color_value(200-i) + '00' for i in range(0,200,200//N)]\n",
    "fig = plt.figure(figsize=(15,13))\n",
    "plots = []\n",
    "\n",
    "# make a grid of subplots\n",
    "n = math.floor(N / math.sqrt(N)) + 1\n",
    "m = N//(n) + 1\n",
    "\n",
    "for i in range(0,N-1):\n",
    "    plots.append(fig.add_subplot(n,m,1+i))\n",
    "    plots[i].scatter(x,y, alpha=0.3)\n",
    "    plots[i].plot(x, predictions[i], c=colors[i])\n",
    "    plots[i].set_title(str(i+1))\n",
    "    plots[i].get_xaxis().set_visible(False)\n",
    "    plots[i].get_yaxis().set_visible(False)\n",
    "    \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's also isolate a few models and show them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def quick_plot(i):\n",
    "    Xi = df[list(df.columns[:i])].values\n",
    "    mi = LinearRegression().fit(Xi,y)\n",
    "\n",
    "    predi = mi.predict(Xi)\n",
    "\n",
    "    plt.scatter(x,y, alpha=0.7)\n",
    "    plt.plot(x, predi, c=colors[i-1])\n",
    "    plt.plot(x, y_true, 'k--')\n",
    "    title = plt.title(\"degree-{} polynomial\".format(i))\n",
    "    plt.setp(title, color=colors[i-1])  \n",
    "    plt.show()\n",
    "\n",
    "quick_plot(2)\n",
    "quick_plot(4)\n",
    "quick_plot(12)\n",
    "quick_plot(36)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's clear that the too-low-dimensional models are a bad fit, they're **underfit**.  The too-high-dimensional models certainly appear to be **overfit**.  How can we justify that with our data?  They technically have less error than the degree-4 model.  Let's see that: we'll plot the error against the \"model flexibility\", _i.e._ the degree of the polynomial in the fit. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def rmse(preds, true_y=y):\n",
    "    \"\"\"The error of the model.\"\"\"\n",
    "    return math.sqrt(mean((true_y-preds)**2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_size = list(range(1,N))\n",
    "\n",
    "training_rmse = []\n",
    "for i in model_size:\n",
    "    training_rmse.append(rmse(predictions[i-1]))\n",
    "\n",
    "plt.plot(model_size, training_rmse, label=\"Training Error\")\n",
    "plt.xlabel(\"Model Flexibility\")\n",
    "plt.ylabel(\"RMSE\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So a few things I note from this is that there appears to be a sharp change in error once we start being \"appropriately biased\" in our dataset.  However, the error of my model still does decrease.  This seems a bit silly, but I could in theory create a model that's so flexible it can fit all the points.  These variations clearly won't hold up when we start seeing data that we haven't seen before."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test Data \n",
    "\n",
    "Let's generate some new data from the same distribution, and see which models do best at predicting it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x_test = np.random.randn(500)\n",
    "x_test = np.array([i for i in x_test if abs(i) <= 1])\n",
    "y_test = true_model(x_test) + (NOISE * np.random.randn(x_test.size))\n",
    "\n",
    "df_test = pd.DataFrame({'x':x_test},index=list(range(x_test.size)))\n",
    "for i in range(2,N+1):\n",
    "    df_test['x^' + str(i)] = x_test**i\n",
    "\n",
    "test_Xs = [df_test[list(df_test.columns[:i])].values for i in range(1,df_test.shape[1])]\n",
    "test_predictions = [model.predict(X) for X,model in zip(test_Xs,models)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a quick look at our training and test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X4 = df[list(df.columns[:4])].values\n",
    "m4 = LinearRegression().fit(X4,y)\n",
    "\n",
    "pred4 = m4.predict(X4)\n",
    "\n",
    "plt.scatter(x, y, label='training data', alpha=0.7)\n",
    "plt.scatter(x_test, y_test, label='test data', alpha=0.7)\n",
    "plt.plot(x, pred4, c=colors[3])\n",
    "plt.legend(loc='lower right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clearly, our training and test data come from the same distribution.  However, let's look at our models' predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "test_rmse = []\n",
    "for i in model_size:\n",
    "    test_rmse.append(rmse(test_predictions[i-1], true_y=y_test))\n",
    "\n",
    "fig = plt.figure(figsize=(8,6))\n",
    "plt.plot(model_size[:-15], training_rmse[:-15], label=\"Training Error\")\n",
    "plt.plot(model_size[:-15], test_rmse[:-15], c='g', label=\"Test Error\")\n",
    "plt.xlabel(\"Model Flexibility\")\n",
    "plt.ylabel(\"RMSE\")\n",
    "plt.legend(loc='upper right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that there is an overall difference between the training and the test error: that's because the test data was chosen to be less noisey than the training data, and it's not a big deal.  The key insight is that there is a \"sweet spot\" of lowest error in the test data: too little flexible, and our model's bias will create more error.  Too much flexibility, and we start fitting the noise in our dataset, and our model's bias will create more error.  Thus, we want to choose a model which attempts to split the difference to try to find the smallest _test_ error."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
